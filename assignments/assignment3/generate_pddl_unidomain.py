#!/usr/bin/env python3
"""
UniDomain: PDDL Domain Pre-training from Robot Manipulation Demonstrations
Based on the UniDomain paper methodology:
1. Energy-based keyframe extraction
2. Atomic domain generation with closed-loop verification
3. Domain fusion (hierarchical merging)
4. Online planning
"""

import json
import os
import subprocess
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
import torch
from PIL import Image
from transformers import (
    BlipForConditionalGeneration,
    BlipProcessor,
    LlavaProcessor,
    LlavaForConditionalGeneration,
    AutoProcessor,
    AutoModelForCausalLM,
)
import clip
from unified_planning.io import PDDLReader
from unified_planning.model import Problem
from unified_planning.shortcuts import OneshotPlanner


class UniDomainGenerator:
    """UniDomain framework for pre-training PDDL domains from demonstrations."""
    
    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.raw_videos_dir = base_dir / "raw_videos"
        self.frames_dir = base_dir / "frames"
        self.annotations_path = base_dir / "droid_language_annotations.json"
        self.atomic_domains_dir = base_dir / "atomic_domains"
        self.atomic_domains_dir.mkdir(exist_ok=True)
        
        # Initialize models
        print("üîß Loading VLM models...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # VLM model selection (prioritize advanced models)
        self.vlm_mode = os.environ.get("VLM_MODE", "llava")  # Options: "llava", "qwen", "blip"
        
        # Load advanced VLM models (LLaVA-1.6 is much better than BLIP)
        self.llava_processor = None
        self.llava_model = None
        self.qwen_processor = None
        self.qwen_model = None
        self.blip_processor = None
        self.blip_model = None
        
        # Try LLaVA-1.6 first (best for detailed understanding)
        if self.vlm_mode in ["llava", "auto"]:
            try:
                print("   üöÄ Loading LLaVA-1.6 (advanced VLM)...")
                self.llava_processor = LlavaProcessor.from_pretrained("llava-hf/llava-1.6-vicuna-7b-hf")
                self.llava_model = LlavaForConditionalGeneration.from_pretrained(
                    "llava-hf/llava-1.6-vicuna-7b-hf",
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    device_map="auto" if self.device == "cuda" else None,
                )
                if self.device == "cpu" or self.llava_model.device.type == "cpu":
                    self.llava_model = self.llava_model.to(self.device)
                self.llava_model.eval()
                print("   ‚úÖ LLaVA-1.6 loaded (advanced VLM)")
                self.vlm_mode = "llava"
            except Exception as e:
                print(f"   ‚ö†Ô∏è  LLaVA-1.6 failed to load: {e}")
                print("   üí° Falling back to Qwen-VL or BLIP...")
                self.vlm_mode = "qwen" if self.vlm_mode == "llava" else "blip"
        
        # Try Qwen-VL as fallback (excellent for vision tasks)
        if self.vlm_mode in ["qwen", "auto"] and self.llava_model is None:
            try:
                print("   üöÄ Loading Qwen-VL (Alibaba's advanced VLM)...")
                self.qwen_processor = AutoProcessor.from_pretrained("Qwen/Qwen-VL", trust_remote_code=True)
                self.qwen_model = AutoModelForCausalLM.from_pretrained(
                    "Qwen/Qwen-VL",
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    device_map="auto" if self.device == "cuda" else None,
                    trust_remote_code=True,
                )
                if self.device == "cpu" or self.qwen_model.device.type == "cpu":
                    self.qwen_model = self.qwen_model.to(self.device)
                self.qwen_model.eval()
                print("   ‚úÖ Qwen-VL loaded (advanced VLM)")
                self.vlm_mode = "qwen"
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Qwen-VL failed to load: {e}")
                print("   üí° Falling back to BLIP...")
                self.vlm_mode = "blip"
        
        # Fallback to BLIP (base model, always available)
        if self.vlm_mode == "blip" or (self.llava_model is None and self.qwen_model is None):
            print("   üì¶ Loading BLIP (fallback)...")
            self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
            self.blip_model = BlipForConditionalGeneration.from_pretrained(
                "Salesforce/blip-image-captioning-base"
            ).to(self.device)
            self.blip_model.eval()
            print("   ‚úÖ BLIP loaded (fallback)")
            self.vlm_mode = "blip"
        
        # CLIP for object detection (upgraded to SigLIP if available)
        # Always initialize CLIP as fallback, even if SigLIP is available
        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)
        print(f"   ‚úÖ CLIP loaded on {self.device}")
        
        self.use_siglip = False
        self.siglip_processor = None
        self.siglip_model = None
        try:
            # Try SigLIP for better object detection (optional upgrade)
            from transformers import AutoImageProcessor, AutoModel
            print("   üéØ Loading SigLIP (advanced object detection, optional upgrade)...")
            self.siglip_processor = AutoImageProcessor.from_pretrained("google/siglip-base-patch16-224")
            self.siglip_model = AutoModel.from_pretrained("google/siglip-base-patch16-224").to(self.device)
            self.siglip_model.eval()
            self.use_siglip = True
            print("   ‚úÖ SigLIP loaded (will use for better accuracy)")
        except Exception as e:
            # CLIP already loaded, just continue with it
            print(f"   üí° SigLIP not available ({e}), using CLIP only")
            self.use_siglip = False
        
        # Load annotations
        self.annotations = self.load_annotations()
        
        # Atomic domains storage
        self.atomic_domains: List[Dict] = []
        
    def load_annotations(self) -> Dict:
        """Load and parse language annotations."""
        print("üìñ Loading language annotations...")
        try:
            with open(self.annotations_path, 'r') as f:
                content = f.read()
            if '<<<<<<< HEAD' in content:
                start_idx = content.find('{')
                end_idx = content.rfind('}', 0, content.find('>>>>>>>'))
                if start_idx != -1 and end_idx != -1:
                    content = content[start_idx:end_idx + 1]
            annotations = json.loads(content)
            print(f"   ‚úÖ Loaded {len(annotations)} annotations")
            return annotations
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error loading annotations: {e}")
            return {}
    
    def extract_all_frames(self, video_path: Path, output_dir: Path) -> List[Path]:
        """Extract all frames from video for energy calculation."""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Extract all frames using ffmpeg
        output_pattern = output_dir / "%06d.jpg"
        cmd = [
            'ffmpeg', '-i', str(video_path),
            '-vf', 'fps=30',  # High FPS for energy calculation
            '-q:v', '2',
            str(output_pattern),
            '-y'
        ]
        subprocess.run(cmd, capture_output=True, check=True)
        frames = sorted(output_dir.glob("*.jpg"))
        return frames
    
    def energy_based_keyframe_extraction(self, frames: List[Path], window_size: int = 30) -> List[int]:
        """
        Energy-based keyframe extraction using grayscale intensity changes.
        Based on UniDomain paper Equation 1-2.
        """
        if len(frames) < 2:
            return [0] if frames else []
        
        # Compute energy for each frame
        energies = []
        for frame_path in frames:
            try:
                img = Image.open(frame_path).convert('L')  # Grayscale
                img_array = np.array(img)
                # Energy = sum of squared pixel values (Equation 1)
                energy = np.sum(img_array ** 2)
                energies.append(energy)
            except Exception as e:
                print(f"      ‚ö†Ô∏è  Error processing {frame_path.name}: {e}")
                energies.append(0)
        
        # Find local extrema (Equation 2)
        keyframe_indices = []
        for i in range(len(energies)):
            window_start = max(0, i - window_size)
            window_end = min(len(energies), i + window_size + 1)
            window_energies = energies[window_start:window_end]
            
            if len(window_energies) == 0:
                continue
                
            # Check if current frame is local maximum or minimum
            if energies[i] == max(window_energies) or energies[i] == min(window_energies):
                keyframe_indices.append(i)
        
        # Always include first and last frames
        if len(keyframe_indices) == 0 or keyframe_indices[0] != 0:
            keyframe_indices.insert(0, 0)
        if keyframe_indices[-1] != len(frames) - 1:
            keyframe_indices.append(len(frames) - 1)
        
        # Remove duplicates and sort
        keyframe_indices = sorted(list(set(keyframe_indices)))
        
        return keyframe_indices
    
    def caption_image(self, image_path: Path, prompt: Optional[str] = None) -> str:
        """
        Generate detailed caption for an image using the best available VLM.
        Uses advanced models (LLaVA/Qwen) for better understanding.
        """
        image = Image.open(image_path).convert('RGB')
        
        # Use LLaVA-1.6 if available (best for detailed understanding)
        if self.llava_model is not None:
            if prompt is None:
                prompt = (
                    "USER: <image>\n"
                    "Describe this image in detail. Focus on: robot actions, objects (blocks, containers), "
                    "spatial relationships, object positions (on table, in container, stacked), colors, "
                    "and any manipulation states. Be specific and detailed.\n"
                    "ASSISTANT:"
                )
            
            inputs = self.llava_processor(prompt, image, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                generate_ids = self.llava_model.generate(
                    **inputs,
                    max_new_tokens=200,
                    do_sample=False,
                )
            
            caption = self.llava_processor.batch_decode(
                generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]
            
            # Extract assistant response
            if "ASSISTANT:" in caption:
                caption = caption.split("ASSISTANT:")[-1].strip()
            
            return caption
        
        # Use Qwen-VL if available
        elif self.qwen_model is not None:
            if prompt is None:
                prompt = "Describe this image in detail, focusing on robot actions, objects, and spatial relationships."
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt},
                    ],
                }
            ]
            
            text = self.qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = self.qwen_processor.process_vision_info(messages)
            
            inputs = self.qwen_processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            ).to(self.device)
            
            with torch.no_grad():
                generate_ids = self.qwen_model.generate(**inputs, max_new_tokens=200)
            
            generate_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generate_ids)
            ]
            caption = self.qwen_processor.batch_decode(
                generate_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]
            
            return caption
        
        # Fallback to BLIP
        else:
            inputs = self.blip_processor(image, return_tensors="pt").to(self.device)
            with torch.no_grad():
                out = self.blip_model.generate(**inputs, max_length=150)
            caption = self.blip_processor.decode(out[0], skip_special_tokens=True)
            return caption
    
    def detect_objects_with_clip(self, image_path: Path, candidate_objects: List[str]) -> Dict[str, float]:
        """Detect objects in image using CLIP or SigLIP (if available)."""
        image = Image.open(image_path).convert('RGB')
        
        # Use SigLIP if available (better than CLIP)
        if self.use_siglip and self.siglip_model is not None:
            try:
                from transformers import CLIPProcessor, CLIPModel
                # Use CLIP processor for text (SigLIP doesn't have text encoder)
                clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
                
                # Process image with SigLIP
                siglip_inputs = self.siglip_processor(image, return_tensors="pt").to(self.device)
                # Process text with CLIP
                text_inputs = clip_processor(text=candidate_objects, return_tensors="pt", padding=True, truncation=True).to(self.device)
                
                with torch.no_grad():
                    # Get image features from SigLIP
                    siglip_outputs = self.siglip_model(**siglip_inputs)
                    image_features = siglip_outputs.pooler_output if hasattr(siglip_outputs, 'pooler_output') else siglip_outputs.last_hidden_state.mean(dim=1)
                    
                    # Get text features from CLIP
                    text_features = clip_model.get_text_features(**text_inputs)
                    
                    # Normalize and compute similarity
                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                    
                    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
                
                results = {}
                for i, obj in enumerate(candidate_objects):
                    results[obj] = float(similarity[0][i])
                
                return results
            except Exception as e:
                # Fallback to CLIP if SigLIP fails
                pass
        
        # Use CLIP (fallback or if SigLIP not available)
        image_tensor = self.clip_preprocess(image).unsqueeze(0).to(self.device)
        text_tokens = clip.tokenize(candidate_objects).to(self.device)
        
        with torch.no_grad():
            image_features = self.clip_model.encode_image(image_tensor)
            text_features = self.clip_model.encode_text(text_tokens)
            
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        
        results = {}
        for i, obj in enumerate(candidate_objects):
            results[obj] = float(similarity[0][i])
        
        return results
    
    def generate_atomic_domain_from_keyframes(
        self, 
        episode_id: str,
        keyframes: List[Path],
        instruction: str
    ) -> Optional[Dict]:
        """
        Generate atomic PDDL domain from keyframes using VLM analysis.
        Based on UniDomain Section 4.2.
        """
        if len(keyframes) < 2:
            return None
        
        print(f"   üî¨ Generating atomic domain from {len(keyframes)} keyframes...")
        
        # Analyze keyframes with VLM
        keyframe_captions = []
        keyframe_objects = []
        
        candidate_objects = [
            "red block", "green block", "blue block", "orange block", "yellow block",
            "bowl", "cup", "container", "mug", "jar",
            "robot arm", "robot gripper", "table", "surface"
        ]
        
        print(f"      üìù Captioning {len(keyframes)} keyframes with {self.vlm_mode.upper()}...")
        for i, kf in enumerate(keyframes):
            caption = self.caption_image(kf)
            objects = self.detect_objects_with_clip(kf, candidate_objects)
            keyframe_captions.append(caption)
            keyframe_objects.append(objects)
            if i == 0 or i == len(keyframes) - 1:
                print(f"         [{i+1}/{len(keyframes)}] {caption[:80]}...")
        
        # Analyze transitions between keyframes
        operators = []
        predicates = set()
        
        # Initial and final states
        initial_caption = keyframe_captions[0]
        final_caption = keyframe_captions[-1]
        initial_objects = keyframe_objects[0]
        final_objects = keyframe_objects[-1]
        
        # Infer operator from initial->final transition
        operator_name = self._infer_operator_name(instruction, initial_caption, final_caption)
        preconditions, effects = self._infer_preconditions_and_effects(
            initial_caption, final_caption, initial_objects, final_objects
        )
        
        # Extract predicates from preconditions and effects
        for pred in preconditions + effects:
            predicates.add(self._extract_predicate_name(pred))
        
        operators.append({
            'name': operator_name,
            'preconditions': preconditions,
            'effects': effects
        })
        
        # Generate PDDL domain structure
        atomic_domain = {
            'episode_id': episode_id,
            'instruction': instruction,
            'operators': operators,
            'predicates': list(predicates),
            'keyframes': len(keyframes),
            'captions': keyframe_captions
        }
        
        return atomic_domain
    
    def _infer_operator_name(self, instruction: str, initial: str, final: str) -> str:
        """Infer operator name from instruction and state changes."""
        instruction_lower = instruction.lower()
        initial_lower = initial.lower()
        final_lower = final.lower()
        
        # Check for common manipulation verbs
        if any(word in instruction_lower for word in ['put', 'place', 'drop']):
            if any(word in final_lower for word in ['in', 'inside', 'container']):
                return 'put_in_container'
            elif any(word in final_lower for word in ['on', 'top', 'stack']):
                return 'stack_block'
        elif any(word in instruction_lower for word in ['remove', 'take out', 'take from']):
            return 'remove_from_container'
        elif any(word in instruction_lower for word in ['stack', 'stacking']):
            return 'stack_block'
        elif any(word in instruction_lower for word in ['pick', 'grasp']):
            return 'pick_block'
        
        # Default based on visual changes
        if 'in' in final_lower and 'in' not in initial_lower:
            return 'put_in_container'
        elif 'on' in final_lower and 'on' not in initial_lower:
            return 'stack_block'
        else:
            return 'manipulate_object'
    
    def _infer_preconditions_and_effects(
        self, 
        initial: str, 
        final: str,
        initial_objects: Dict[str, float],
        final_objects: Dict[str, float]
    ) -> Tuple[List[str], List[str]]:
        """Infer preconditions and effects from state changes using PDDL variables."""
        preconditions = []
        effects = []
        
        initial_lower = initial.lower()
        final_lower = final.lower()
        
        # Detect block colors
        block_colors = ['red', 'green', 'blue', 'orange', 'yellow']
        detected_blocks = []
        for color in block_colors:
            if any(f'{color} block' in obj for obj in initial_objects.keys() if initial_objects[obj] > 0.1):
                detected_blocks.append(color)
        
        # Detect containers
        container_words = ['bowl', 'cup', 'container', 'mug', 'jar']
        has_container = any(
            word in initial_lower or word in final_lower or 
            any(word in obj for obj in initial_objects.keys() if initial_objects[obj] > 0.1)
            for word in container_words
        )
        
        # Use PDDL variables: ?b for block, ?r for robot, ?c for container
        # Infer based on state changes
        if 'in' in final_lower or 'inside' in final_lower:
            if 'in' not in initial_lower and 'inside' not in initial_lower:
                # Block moved into container
                if detected_blocks and has_container:
                    preconditions.extend(['(on-table ?b)', '(clear ?b)', '(empty ?c)'])
                    effects.extend(['(in ?b ?c)', '(not (on-table ?b))'])
        elif 'on' in final_lower and ('on top' in final_lower or 'stacked' in final_lower):
            # Stacking detected - need two blocks
            if len(detected_blocks) >= 2:
                # For stacking, we need a different action signature
                # Use ?b1 and ?b2 for two blocks
                preconditions.extend(['(on-table ?b)', '(clear ?b)'])
                effects.extend(['(on ?b ?b)', '(not (on-table ?b))'])
        elif 'remove' in final_lower or 'take out' in final_lower:
            # Removal from container
            if detected_blocks and has_container:
                preconditions.extend(['(in ?b ?c)'])
                effects.extend(['(on-table ?b)', '(not (in ?b ?c))', '(holding ?r ?b)'])
        
        # Default preconditions/effects if none detected
        if not preconditions:
            preconditions = ['(on-table ?b)', '(clear ?b)']
        if not effects:
            effects = ['(holding ?r ?b)']
        
        return preconditions, effects
    
    def _extract_predicate_name(self, predicate: str) -> str:
        """Extract predicate name from PDDL predicate string."""
        # Remove parentheses and split
        pred = predicate.strip('()')
        parts = pred.split()
        if len(parts) > 0:
            return parts[0]
        return 'unknown'
    
    def verify_atomic_domain(self, atomic_domain: Dict, max_iterations: int = 5) -> Optional[Dict]:
        """
        Closed-loop verification of atomic domain.
        Based on UniDomain Section 4.2 (Solvability Check).
        """
        print(f"   üîÑ Verifying atomic domain (max {max_iterations} iterations)...")
        
        # Generate test problem from domain
        test_problem = self._generate_test_problem(atomic_domain)
        
        if not test_problem:
            return None
        
        # Try to validate with PDDL reader
        try:
            # Create temporary domain and problem files
            temp_domain = self.atomic_domains_dir / f"temp_domain_{atomic_domain['episode_id'].replace('+', '_').replace('-', '_')}.pddl"
            temp_problem = self.atomic_domains_dir / f"temp_problem_{atomic_domain['episode_id'].replace('+', '_').replace('-', '_')}.pddl"
            
            self._write_atomic_domain_pddl(atomic_domain, temp_domain)
            self._write_test_problem_pddl(test_problem, temp_problem, atomic_domain['episode_id'].replace('+', '_').replace('-', '_'))
            
            # Try to parse with unified_planning
            reader = PDDLReader()
            problem = reader.parse_problem(str(temp_domain), str(temp_problem))
            
            # Try to solve (if planner available)
            try:
                with OneshotPlanner(problem_kind=problem.kind) as planner:
                    result = planner.solve(problem)
                    if result.plan:
                        print(f"      ‚úÖ Domain verified (solvable)")
                        atomic_domain['verified'] = True
                        atomic_domain['solvability_score'] = 1.0
                        return atomic_domain
                    else:
                        print(f"      ‚ö†Ô∏è  Domain not solvable")
                        atomic_domain['verified'] = False
                        atomic_domain['solvability_score'] = 0.0
                        return atomic_domain
            except Exception as solve_error:
                # If solving fails, at least verify syntax
                print(f"      ‚ö†Ô∏è  Syntax valid but solving failed: {solve_error}")
                atomic_domain['verified'] = True  # Syntax is valid
                atomic_domain['solvability_score'] = 0.5
                atomic_domain['solve_error'] = str(solve_error)
                return atomic_domain
                    
        except Exception as e:
            import traceback
            error_msg = str(e)
            error_detail = traceback.format_exc()
            print(f"      ‚ö†Ô∏è  Verification failed: {error_msg}")
            if "invalid expression" in error_msg.lower():
                print(f"      üí° Hint: Check domain and problem files for syntax errors")
            atomic_domain['verified'] = False
            atomic_domain['error'] = error_msg
            return atomic_domain
    
    def _generate_test_problem(self, atomic_domain: Dict) -> Optional[Dict]:
        """Generate a test problem from atomic domain."""
        if not atomic_domain.get('operators'):
            return None
        
        op = atomic_domain['operators'][0]
        preconditions = op.get('preconditions', [])
        effects = op.get('effects', [])
        
        # Convert variables to concrete objects for test problem
        # Variables: ?b -> block1, ?r -> robot1, ?c -> bowl1
        initial_concrete = []
        for pred in preconditions:
            pred_concrete = pred.replace('?b', 'block1').replace('?r', 'robot1').replace('?c', 'bowl1')
            initial_concrete.append(pred_concrete)
        
        goal_concrete = []
        for eff in effects:
            eff_concrete = eff.replace('?b', 'block1').replace('?r', 'robot1').replace('?c', 'bowl1')
            goal_concrete.append(eff_concrete)
        
        # Simple test problem generation
        test_problem = {
            'objects': ['block1', 'robot1'],
            'initial': initial_concrete,
            'goal': goal_concrete
        }
        
        # Add container if needed
        if any('?c' in str(eff) or 'in' in str(eff) for eff in effects):
            test_problem['objects'].append('bowl1')
        
        return test_problem
    
    def _write_atomic_domain_pddl(self, atomic_domain: Dict, output_path: Path):
        """Write atomic domain to PDDL file."""
        operators = atomic_domain.get('operators', [])
        predicates = set()
        
        # Extract predicates from operators with their full signatures
        predicate_signatures = {}
        for op in operators:
            for pred in op.get('preconditions', []):
                pred_name = self._extract_predicate_name(pred)
                pred_sig = self._extract_predicate_signature(pred)
                predicate_signatures[pred_name] = pred_sig
                predicates.add(pred_name)
            for pred in op.get('effects', []):
                pred_name = self._extract_predicate_name(pred)
                pred_sig = self._extract_predicate_signature(pred)
                predicate_signatures[pred_name] = pred_sig
                predicates.add(pred_name)
        
        pddl_content = f"""(define (domain {atomic_domain['episode_id'].replace('+', '_').replace('-', '_')})
    (:requirements :strips :typing)
    (:types
        block container robot surface - object
    )
    (:predicates
"""
        # Standard predicates for robot manipulation
        standard_preds = [
            "(holding ?r - robot ?b - block)",
            "(on ?o1 - block ?o2 - object)",
            "(clear ?o - block)",
            "(in ?o - block ?c - container)",
            "(on-table ?o - block)",
            "(empty ?c - container)",
            "(open ?c - container)",
            "(closed ?c - container)"
        ]
        
        for pred in standard_preds:
            pddl_content += f"        {pred}\n"
        
        pddl_content += "    )\n"
        
        # Write operators
        for op in operators:
            preconditions = op.get('preconditions', [])
            effects = op.get('effects', [])
            
            # Filter out invalid predicates and ensure variables are used
            valid_preconditions = []
            for p in preconditions:
                # Check if it's a valid predicate
                if any(sp in p for sp in ['holding', 'on', 'clear', 'in', 'on-table', 'empty', 'open', 'closed']):
                    # Replace concrete object names with variables
                    p_vars = p.replace('block1', '?b').replace('block2', '?b').replace('robot1', '?r').replace('bowl1', '?c').replace('cup1', '?c')
                    valid_preconditions.append(p_vars)
            
            valid_effects = []
            for e in effects:
                # Check if it's a valid predicate
                if any(sp in e for sp in ['holding', 'on', 'clear', 'in', 'on-table', 'empty', 'open', 'closed']):
                    # Replace concrete object names with variables
                    e_vars = e.replace('block1', '?b').replace('block2', '?b').replace('robot1', '?r').replace('bowl1', '?c').replace('cup1', '?c')
                    valid_effects.append(e_vars)
            
            if not valid_preconditions:
                valid_preconditions = ['(on-table ?b)', '(clear ?b)']
            if not valid_effects:
                valid_effects = ['(holding ?r ?b)']
            
            pddl_content += f"""
    (:action {op['name']}
        :parameters (?r - robot ?b - block ?c - container)
        :precondition (and {' '.join(valid_preconditions)})
        :effect (and {' '.join(valid_effects)})
    )
"""
        
        pddl_content += ")\n"
        
        with open(output_path, 'w') as f:
            f.write(pddl_content)
    
    def _extract_predicate_signature(self, predicate: str) -> str:
        """Extract predicate signature from PDDL predicate string."""
        # Remove parentheses and not
        pred = predicate.strip('()')
        if pred.startswith('not'):
            pred = pred[3:].strip()
        
        # Extract predicate name and parameters
        parts = pred.split()
        if len(parts) == 0:
            return "unknown"
        
        pred_name = parts[0]
        # Determine arity from remaining parts
        params = parts[1:] if len(parts) > 1 else []
        
        return pred_name
    
    def _write_test_problem_pddl(self, test_problem: Dict, output_path: Path, domain_name: str):
        """Write test problem to PDDL file."""
        objects = test_problem['objects']
        
        # Separate objects by type
        blocks = [obj for obj in objects if 'block' in obj]
        containers = [obj for obj in objects if 'bowl' in obj or 'cup' in obj or 'container' in obj]
        robots = [obj for obj in objects if 'robot' in obj]
        others = [obj for obj in objects if obj not in blocks + containers + robots]
        
        objects_str = ""
        if blocks:
            objects_str += f"{' '.join(blocks)} - block "
        if containers:
            objects_str += f"{' '.join(containers)} - container "
        if robots:
            objects_str += f"{' '.join(robots)} - robot "
        if others:
            objects_str += f"{' '.join(others)} - object"
        
        initial_str = ' '.join(test_problem['initial'])
        goal_str = ' '.join(test_problem['goal'])
        
        pddl_content = f"""(define (problem test)
    (:domain {domain_name})
    (:objects {objects_str})
    (:init {initial_str})
    (:goal (and {goal_str}))
)
"""
        with open(output_path, 'w') as f:
            f.write(pddl_content)
    
    def process_video_for_atomic_domain(self, episode_id: str, video_path: Path) -> Optional[Dict]:
        """
        Process a single video to generate atomic domain.
        Implements UniDomain Section 4 (Domain Pretraining).
        """
        print(f"\n{'='*70}")
        print(f"üìπ Processing: {episode_id}")
        print(f"   Video: {video_path.name}")
        
        # Step 1: Extract all frames
        frames_output = self.frames_dir / episode_id
        frames_output.mkdir(parents=True, exist_ok=True)
        
        print(f"   üé¨ Extracting frames for energy calculation...")
        all_frames = self.extract_all_frames(video_path, frames_output)
        print(f"      ‚úÖ Extracted {len(all_frames)} frames")
        
        # Step 2: Energy-based keyframe extraction
        print(f"   ‚ö° Energy-based keyframe extraction...")
        keyframe_indices = self.energy_based_keyframe_extraction(all_frames, window_size=30)
        keyframes = [all_frames[i] for i in keyframe_indices]
        print(f"      ‚úÖ Selected {len(keyframes)} keyframes: {keyframe_indices}")
        
        # Step 3: Get instruction
        annot = self.annotations.get(episode_id, {})
        if isinstance(annot, dict):
            instruction = annot.get('language_instruction1', '')
        else:
            instruction = ''
        print(f"   üìù Instruction: '{instruction}'")
        
        # Step 4: Generate atomic domain from keyframes
        atomic_domain = self.generate_atomic_domain_from_keyframes(
            episode_id, keyframes, instruction
        )
        
        if not atomic_domain:
            print(f"   ‚ùå Failed to generate atomic domain")
            return None
        
        # Step 5: Verify atomic domain (closed-loop verification)
        verified_domain = self.verify_atomic_domain(atomic_domain)
        
        if verified_domain and verified_domain.get('verified', False):
            print(f"   ‚úÖ Atomic domain generated and verified")
            self.atomic_domains.append(verified_domain)
            
            # Save atomic domain
            domain_file = self.atomic_domains_dir / f"{episode_id.replace('+', '_').replace('-', '_')}.json"
            with open(domain_file, 'w') as f:
                json.dump(verified_domain, f, indent=2)
            
            return verified_domain
        else:
            print(f"   ‚ö†Ô∏è  Atomic domain generated but not verified")
            return atomic_domain
    
    def process_all_videos(self):
        """Process all videos to generate atomic domains."""
        print("="*70)
        print("üöÄ UNIDOMAIN: Domain Pretraining from Demonstrations")
        print("="*70)
        
        # Find all video folders
        episode_folders = [d for d in self.raw_videos_dir.iterdir() if d.is_dir()]
        print(f"\nüìÅ Found {len(episode_folders)} video episodes")
        
        # Process each video
        for episode_folder in episode_folders:
            episode_id = episode_folder.name
            
            # Find video file
            video_path = None
            for ext in ['*.mp4', '*.MP4', '*.avi', '*.mov']:
                videos = list(episode_folder.rglob(ext))
                if videos:
                    video_path = videos[0]
                    break
            
            if not video_path:
                print(f"   ‚ö†Ô∏è  No video found for {episode_id}, skipping...")
                continue
            
            try:
                self.process_video_for_atomic_domain(episode_id, video_path)
            except Exception as e:
                print(f"   ‚ùå Error processing {episode_id}: {e}")
                import traceback
                traceback.print_exc()
        
        print("\n" + "="*70)
        print(f"‚úÖ Generated {len(self.atomic_domains)} atomic domains")
        print("="*70)
        
        # Generate unified domain from atomic domains
        if self.atomic_domains:
            self.generate_unified_domain()
    
    def generate_unified_domain(self):
        """Generate unified domain from all atomic domains."""
        print("\nüîó Generating unified domain from atomic domains...")
        
        # Aggregate predicates and operators
        all_predicates = set()
        all_operators = []
        
        for atomic_domain in self.atomic_domains:
            all_predicates.update(atomic_domain.get('predicates', []))
            all_operators.extend(atomic_domain.get('operators', []))
        
        print(f"   üìä Unified domain: {len(all_predicates)} predicates, {len(all_operators)} operators")
        
        # Write unified domain
        unified_domain_path = self.base_dir / "unified_domain.pddl"
        self._write_unified_domain_pddl(all_predicates, all_operators, unified_domain_path)
        
        print(f"   ‚úÖ Unified domain saved: {unified_domain_path}")
    
    def _write_unified_domain_pddl(self, predicates: Set[str], operators: List[Dict], output_path: Path):
        """Write unified domain to PDDL file."""
        pddl_content = """(define (domain unified-robot-manipulation)
    (:requirements :strips :typing)
    (:types
        block container robot surface - object
    )
    (:predicates
"""
        # Standard predicates for robot manipulation (same as atomic domains)
        standard_preds = [
            "(holding ?r - robot ?b - block)",
            "(on ?o1 - block ?o2 - object)",
            "(clear ?o - block)",
            "(in ?o - block ?c - container)",
            "(on-table ?o - block)",
            "(empty ?c - container)",
            "(open ?c - container)",
            "(closed ?c - container)"
        ]
        
        for pred in standard_preds:
            pddl_content += f"        {pred}\n"
        
        pddl_content += "    )\n"
        
        # Write unique operators with merged preconditions/effects
        seen_operators = {}
        for op in operators:
            op_name = op['name']
            if op_name not in seen_operators:
                seen_operators[op_name] = {
                    'preconditions': set(op.get('preconditions', [])),
                    'effects': set(op.get('effects', []))
                }
            else:
                # Merge preconditions and effects
                seen_operators[op_name]['preconditions'].update(op.get('preconditions', []))
                seen_operators[op_name]['effects'].update(op.get('effects', []))
        
        for op_name, op_data in seen_operators.items():
            preconditions = list(op_data['preconditions'])
            effects = list(op_data['effects'])
            
            # Filter valid predicates and ensure variables are used
            valid_preconditions = []
            for p in preconditions:
                if any(sp in p for sp in ['holding', 'on', 'clear', 'in', 'on-table', 'empty', 'open', 'closed']):
                    # Replace concrete object names with variables
                    p_vars = p.replace('block1', '?b').replace('block2', '?b').replace('robot1', '?r').replace('bowl1', '?c').replace('cup1', '?c')
                    valid_preconditions.append(p_vars)
            
            valid_effects = []
            for e in effects:
                if any(sp in e for sp in ['holding', 'on', 'clear', 'in', 'on-table', 'empty', 'open', 'closed']):
                    # Replace concrete object names with variables
                    e_vars = e.replace('block1', '?b').replace('block2', '?b').replace('robot1', '?r').replace('bowl1', '?c').replace('cup1', '?c')
                    valid_effects.append(e_vars)
            
            if not valid_preconditions:
                valid_preconditions = ['(on-table ?b)', '(clear ?b)']
            if not valid_effects:
                valid_effects = ['(holding ?r ?b)']
            
            pddl_content += f"""
    (:action {op_name}
        :parameters (?r - robot ?b - block ?c - container)
        :precondition (and {' '.join(valid_preconditions)})
        :effect (and {' '.join(valid_effects)})
    )
"""
        
        pddl_content += ")\n"
        
        with open(output_path, 'w') as f:
            f.write(pddl_content)


def main():
    base_dir = Path(__file__).parent
    generator = UniDomainGenerator(base_dir)
    generator.process_all_videos()


if __name__ == "__main__":
    main()

