#!/usr/bin/env python3
"""
PDDL Domain and Problem Generation using Cosmos-Reason1-7B
Sequential video understanding for generating domain.pddl and problem.pddl files.
"""

# Set multiprocessing start method BEFORE any imports that use CUDA/vLLM
# This is required for vLLM to work with CUDA
import multiprocessing
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    # Already set, ignore
    pass

import json
import os
import re
import subprocess
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple
import torch
from PIL import Image
import cv2
from transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig
from qwen_vl_utils import process_vision_info

# Model ID for Cosmos-Reason1-7B (from HuggingFace)
# Based on Qwen2.5-VL-7B-Instruct architecture
COSMOS_MODEL_ID = "nvidia/Cosmos-Reason1-7B"

# Configuration
VIDEO_DIR = "raw_videos"
ANNOTATIONS_FILE = "droid_language_annotations.json"
OUTPUT_DOMAIN_FILE = "domain.pddl"
OUTPUT_PROBLEM_DIR = "problems"
SAMPLE_SIZE = 10  # Number of videos to process
BLOCK_KEYWORDS = ["block", "stack", "place", "put", "pick", "move", "remove"]
FRAME_RATE = 4  # FPS for video analysis (matches model training)

# vLLM configuration (no quantization needed, vLLM handles it)
# vLLM is more efficient for inference than transformers


def load_cosmos_model():
    """
    Loads Cosmos-Reason1-7B model for sequential video understanding using vLLM.
    Based on Qwen2.5-VL-7B-Instruct architecture.
    Reference: https://huggingface.co/nvidia/Cosmos-Reason1-7B
    """
    print("ðŸ”§ Loading Cosmos-Reason1-7B (sequential video understanding)...")
    print(f"   Model: {COSMOS_MODEL_ID}")
    print(f"   Architecture: Qwen2.5-VL-7B-Instruct")
    print(f"   Engine: vLLM (optimized inference)")
    
    if not torch.cuda.is_available():
        raise RuntimeError("vLLM requires CUDA. Please use a GPU-enabled environment.")
    
    try:
        print(f"   ðŸš€ Loading processor and vLLM model...")
        
        # Load processor for message formatting
        processor = AutoProcessor.from_pretrained(
            COSMOS_MODEL_ID,
            trust_remote_code=True
        )
        
        # Workaround for rope_type conflict in vLLM 0.11.0
        # The model config has both legacy 'type=mrope' and modern 'rope_type=default'
        # We need to patch the config.json file in the HuggingFace cache
        try:
            print(f"   ðŸ”§ Patching config to fix rope_type conflict...")
            from huggingface_hub import hf_hub_download
            import tempfile
            
            # Download config.json to a temp file
            config_file = hf_hub_download(
                repo_id=COSMOS_MODEL_ID,
                filename="config.json",
                cache_dir=None
            )
            
            # Read and patch config.json
            with open(config_file, 'r') as f:
                config_dict = json.load(f)
            
            # Remove legacy 'type' field if it conflicts with rope_type
            if 'type' in config_dict and 'rope_type' in config_dict:
                if config_dict.get('type') == 'mrope':
                    print(f"      Removing legacy 'type=mrope' field...")
                    config_dict.pop('type', None)
                    # Write patched config back
                    with open(config_file, 'w') as f:
                        json.dump(config_dict, f, indent=2)
                    print(f"      âœ… Config patched successfully")
            
            # Use the original model path (config is already patched in cache)
            model_path = COSMOS_MODEL_ID
        except Exception as config_error:
            print(f"   âš ï¸  Config patching failed: {config_error}")
            print(f"   ðŸ’¡ Trying direct load (may fail with rope_type error)...")
            model_path = COSMOS_MODEL_ID
        
        # Load model with vLLM (more efficient for inference)
        # limit_mm_per_prompt: limit images/videos per prompt
        # enforce_eager=True: Disable CUDA graphs to avoid WSL2 multiprocessing issues
        llm = LLM(
            model=model_path,
            limit_mm_per_prompt={"image": 10, "video": 10},  # Support up to 10 images/videos
            trust_remote_code=True,
            enforce_eager=True  # Disable CUDA graphs for WSL2 compatibility
        )
        
        print(f"   âœ… Successfully loaded: {COSMOS_MODEL_ID}")
        return processor, llm
    except Exception as e:
        print(f"   âš ï¸  Failed to load: {e}")
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"Failed to load Cosmos-Reason1-7B: {e}")


def extract_sequential_frames(video_path: Path, fps: float = FRAME_RATE) -> List[Image.Image]:
    """Extracts sequential frames from video at specified FPS."""
    frames = []
    cap = cv2.VideoCapture(str(video_path))
    
    if not cap.isOpened():
        print(f"   âš ï¸  Could not open video: {video_path}")
        return frames
    
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    video_fps = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = max(1, int(video_fps / fps))
    
    frame_idx = 0
    extracted_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        if frame_idx % frame_interval == 0:
            # Convert BGR to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_pil = Image.fromarray(frame_rgb)
            frames.append(frame_pil)
            extracted_count += 1
        
        frame_idx += 1
    
    cap.release()
    print(f"      Extracted {extracted_count} frames from {frame_count} total frames")
    return frames


def analyze_sequential_video(
    processor, model, device, video_path: Path, instruction: str
) -> Dict[str, any]:
    """
    Analyzes sequential video to understand what's happening using Cosmos-Reason1-7B.
    Uses vLLM for efficient inference with video input.
    Returns structured data about actions, objects, states, and transitions.
    """
    if not video_path.exists():
        return {"error": f"Video file not found: {video_path}"}
    
    print(f"   ðŸ“¹ Analyzing video with Cosmos-Reason1-7B (vLLM)...")
    
    # Create sequential analysis prompt for Cosmos-Reason1-7B
    # Based on model card: Use chain-of-thought reasoning format
    # Reference: https://huggingface.co/nvidia/Cosmos-Reason1-7B
    prompt_text = f"""Analyze this sequential video showing a robot manipulation task.

Task Instruction: {instruction}

Provide a detailed sequential analysis in JSON format with:
1. "initial_state": Objects, positions, and relationships at the start
2. "goal_state": Desired final configuration  
3. "sequence_of_actions": List of actions performed in order
4. "object_types": Types of objects (e.g., block, container, robot, surface)
5. "predicates": Relationships between objects (e.g., on, in, holding, clear)
6. "state_transitions": How states change between key frames
7. "key_frames": Frame indices where significant state changes occur

Focus on:
- Physical common sense reasoning
- Spatial relationships
- Temporal sequence of actions
- Object manipulation states
- Sequential reasoning about what happens step by step

Format your response as valid JSON."""

    try:
        # Create messages for vLLM (following the example pattern)
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant. Answer the question in the following format: <think>\nyour reasoning\n</think>\n\n<answer>\nyour answer\n</answer>."
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt_text},
                    {
                        "type": "video",
                        "video": f"file://{video_path.absolute()}",  # vLLM expects file:// path
                        "fps": FRAME_RATE,  # FPS=4 as per model card
                    }
                ]
            },
        ]
        
        # Apply chat template
        prompt = processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        
        # Process vision info (handles video extraction)
        image_inputs, video_inputs, video_kwargs = process_vision_info(
            messages, return_video_kwargs=True
        )
        
        # Prepare multi-modal data
        mm_data = {}
        if image_inputs is not None:
            mm_data["image"] = image_inputs
        if video_inputs is not None:
            mm_data["video"] = video_inputs
        
        # Prepare inputs for vLLM
        llm_inputs = {
            "prompt": prompt,
            "multi_modal_data": mm_data,
            "mm_processor_kwargs": video_kwargs,  # FPS will be returned here
        }
        
        # Generate with vLLM
        sampling_params = SamplingParams(
            temperature=0.6,
            top_p=0.95,
            repetition_penalty=1.05,
            max_tokens=4096,  # 4096+ as per model card
        )
        
        outputs = llm.generate([llm_inputs], sampling_params=sampling_params)
        response = outputs[0].outputs[0].text
        
        # Extract JSON from response
        # Try to find JSON in <answer> tags first
        answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
        if answer_match:
            response = answer_match.group(1).strip()
        
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            try:
                analysis = json.loads(json_match.group(0))
                return analysis
            except json.JSONDecodeError:
                # Try to fix common JSON issues
                json_str = json_match.group(0)
                # Replace single quotes with double quotes
                json_str = json_str.replace("'", '"')
                try:
                    analysis = json.loads(json_str)
                    return analysis
                except:
                    pass
        
        # Fallback: return raw response
        return {
            "raw_response": response,
            "instruction": instruction,
            "video_path": str(video_path)
        }
    
    except Exception as e:
        print(f"      âš ï¸  Analysis failed: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e), "instruction": instruction}


def extract_objects_and_predicates(analysis: Dict) -> Tuple[List[str], List[str]]:
    """Extracts object types and predicates from analysis."""
    types = set()
    predicates = set()
    
    # Extract types
    if "object_types" in analysis:
        types.update(analysis["object_types"])
    
    # Extract predicates from initial/goal states
    if "initial_state" in analysis:
        state = analysis["initial_state"]
        if isinstance(state, dict):
            for key in state.keys():
                if isinstance(key, str) and "(" in key:
                    # Extract predicate name
                    pred_match = re.match(r'\((\w+)', key)
                    if pred_match:
                        predicates.add(pred_match.group(1))
    
    if "goal_state" in analysis:
        state = analysis["goal_state"]
        if isinstance(state, dict):
            for key in state.keys():
                if isinstance(key, str) and "(" in key:
                    pred_match = re.match(r'\((\w+)', key)
                    if pred_match:
                        predicates.add(pred_match.group(1))
    
    # Default types if not found
    if not types:
        types = {"block", "container", "robot", "surface", "object"}
    
    # Default predicates if not found
    if not predicates:
        predicates = {"holding", "on", "in", "clear", "on-table", "empty"}
    
    return list(types), list(predicates)


def generate_actions_from_sequence(analysis: Dict) -> List[str]:
    """Generates PDDL actions from sequential action analysis."""
    actions = []
    
    if "sequence_of_actions" in analysis:
        action_sequence = analysis["sequence_of_actions"]
        
        # Map actions to PDDL operators
        for action in action_sequence:
            action_lower = action.lower()
            
            if "pick" in action_lower or "pickup" in action_lower or "grab" in action_lower:
                actions.append("pick-up")
            elif "place" in action_lower or "put" in action_lower:
                actions.append("place")
            elif "stack" in action_lower:
                actions.append("stack")
            elif "put" in action_lower and ("in" in action_lower or "container" in action_lower):
                actions.append("put-in-container")
            elif "remove" in action_lower or "take" in action_lower:
                actions.append("remove-from-container")
    
    # Default actions if none found
    if not actions:
        actions = ["pick-up", "place", "put-in-container", "remove-from-container", "stack"]
    
    return list(set(actions))  # Remove duplicates


def create_pddl_domain(types: List[str], predicates: List[str], actions: List[str]) -> str:
    """Creates PDDL domain file from extracted information."""
    
    # Format types
    types_str = "object"
    subtypes = [t for t in types if t != "object"]
    if subtypes:
        types_str += f"\n\t\t{' '.join(subtypes)} - object"
    
    # Format predicates
    predicate_defs = []
    predicate_map = {
        "holding": "(holding ?r - robot ?b - block)",
        "on": "(on ?o1 - block ?o2 - object)",
        "in": "(in ?o - block ?c - container)",
        "clear": "(clear ?o - block)",
        "on-table": "(on-table ?o - block)",
        "ontable": "(on-table ?o - block)",
        "empty": "(empty ?c - container)",
    }
    
    for pred in predicates:
        if pred in predicate_map:
            predicate_defs.append(predicate_map[pred])
        else:
            # Generic predicate
            predicate_defs.append(f"({pred} ?x - object)")
    
    # Ensure common predicates
    common_predicates = [
        "(holding ?r - robot ?b - block)",
        "(on ?o1 - block ?o2 - object)",
        "(clear ?o - block)",
        "(in ?o - block ?c - container)",
        "(on-table ?o - block)",
        "(empty ?c - container)"
    ]
    for cp in common_predicates:
        if cp not in predicate_defs:
            predicate_defs.append(cp)
    
    # Generate action definitions (simplified)
    action_defs = []
    if "pick-up" in actions:
        action_defs.append("""    (:action pick-up
        :parameters (?r - robot ?b - block)
        :precondition (and (on-table ?b) (clear ?b))
        :effect (and (holding ?r ?b) (not (on-table ?b)) (not (clear ?b)))
    )""")
    
    if "place" in actions:
        action_defs.append("""    (:action place
        :parameters (?r - robot ?b - block ?o - object)
        :precondition (and (holding ?r ?b) (clear ?o))
        :effect (and (on ?b ?o) (not (holding ?r ?b)) (clear ?b))
    )""")
    
    if "put-in-container" in actions or "put-in" in [a.lower() for a in actions]:
        action_defs.append("""    (:action put-in-container
        :parameters (?r - robot ?b - block ?c - container)
        :precondition (and (holding ?r ?b) (empty ?c))
        :effect (and (in ?b ?c) (not (holding ?r ?b)) (not (empty ?c)))
    )""")
    
    if "remove-from-container" in actions or "remove" in [a.lower() for a in actions]:
        action_defs.append("""    (:action remove-from-container
        :parameters (?r - robot ?b - block ?c - container)
        :precondition (and (in ?b ?c))
        :effect (and (holding ?r ?b) (not (in ?b ?c)) (empty ?c))
    )""")
    
    if "stack" in actions:
        action_defs.append("""    (:action stack
        :parameters (?r - robot ?b1 - block ?b2 - block)
        :precondition (and (holding ?r ?b1) (clear ?b2))
        :effect (and (on ?b1 ?b2) (not (holding ?r ?b1)) (clear ?b1))
    )""")
    
    domain_pddl = f"""(define (domain robot-manipulation)
    (:requirements :strips :typing)
    (:types
        {types_str}
    )
    (:predicates
{chr(10).join(f'        {p}' for p in predicate_defs)}
    )
{chr(10).join(action_defs)}
)
"""
    
    return domain_pddl


def create_pddl_problem(
    episode_id: str, 
    analysis: Dict, 
    domain_file: str = "domain.pddl"
) -> str:
    """Creates PDDL problem file from analysis."""
    
    # Extract initial and goal states
    initial_state = analysis.get("initial_state", {})
    goal_state = analysis.get("goal_state", {})
    
    # Format initial state
    init_predicates = []
    if isinstance(initial_state, dict):
        for key, value in initial_state.items():
            if isinstance(key, str) and "(" in key:
                init_predicates.append(key)
    
    # Default initial state
    if not init_predicates:
        init_predicates = [
            "(on-table block1)",
            "(on-table block2)",
            "(clear block1)",
            "(clear block2)",
            "(empty container1)",
        ]
    
    # Format goal state
    goal_predicates = []
    if isinstance(goal_state, dict):
        for key, value in goal_state.items():
            if isinstance(key, str) and "(" in key:
                goal_predicates.append(key)
    
    # Default goal state
    if not goal_predicates:
        goal_predicates = [
            "(on block1 block2)",
        ]
    
    # Extract objects
    objects = ["robot1", "block1", "block2", "container1"]
    
    problem_pddl = f"""(define (problem {episode_id.replace('+', '_')})
    (:domain robot-manipulation)
    (:objects
        {' '.join(objects)} - object
    )
    (:init
{chr(10).join(f'        {p}' for p in init_predicates)}
    )
    (:goal
        (and
{chr(10).join(f'            {p}' for p in goal_predicates)}
        )
    )
)
"""
    
    return problem_pddl


def load_annotations() -> Dict[str, str]:
    """Loads DROID language annotations."""
    annotations_path = Path(ANNOTATIONS_FILE)
    if not annotations_path.exists():
        print(f"   âš ï¸  Annotations file not found: {ANNOTATIONS_FILE}")
        return {}
    
    try:
        with open(annotations_path, 'r') as f:
            content = f.read()
            
            # Handle Git LFS or merge conflict markers
            if content.startswith('version'):
                # Git LFS file
                json_start = content.find('{')
                if json_start > 0:
                    content = content[json_start:]
            
            # Remove merge conflict markers if present
            if '<<<<<<<' in content:
                content = content.split('<<<<<<<')[0]
            
            annotations = json.loads(content)
            print(f"   âœ… Loaded {len(annotations)} annotations")
            return annotations
    except Exception as e:
        print(f"   âš ï¸  Error loading annotations: {e}")
        return {}


def main():
    """Main function to generate PDDL files from sequential video analysis."""
    print("=" * 70)
    print("ðŸš€ PDDL GENERATION WITH COSMOS-REASON1-7B")
    print("   Sequential Video Understanding for Domain & Problem Generation")
    print("=" * 70)
    
    # Load model
    try:
        processor, llm = load_cosmos_model()
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        print(f"   ðŸ’¡ Make sure vLLM and qwen-vl-utils are installed:")
        print(f"      pip install vllm qwen-vl-utils")
        return
    
    # Load annotations
    print("\nðŸ“– Loading annotations...")
    annotations = load_annotations()
    
    # Find videos
    print(f"\nðŸ“ Finding videos in {VIDEO_DIR}...")
    video_dir = Path(VIDEO_DIR)
    if not video_dir.exists():
        print(f"   âŒ Video directory not found: {VIDEO_DIR}")
        return
    
    video_episodes = [d for d in video_dir.iterdir() if d.is_dir()]
    print(f"   âœ… Found {len(video_episodes)} video episodes")
    
    # Process videos
    all_analyses = []
    domain_types = set()
    domain_predicates = set()
    domain_actions = set()
    
    for episode_dir in video_episodes[:SAMPLE_SIZE]:
        episode_id = episode_dir.name
        print(f"\n{'=' * 70}")
        print(f"ðŸ“¹ Processing: {episode_id}")
        
        # Find video file
        video_files = list(episode_dir.glob("*.mp4"))
        if not video_files:
            print(f"   âš ï¸  No video file found in {episode_dir}")
            continue
        
        video_path = video_files[0]
        print(f"   Video: {video_path.name}")
        
        # Get instruction
        instruction = annotations.get(episode_id, "Perform robot manipulation task")
        print(f"   Instruction: {instruction}")
        
        # Analyze sequential video directly (vLLM handles video processing)
        analysis = analyze_sequential_video(processor, llm, video_path, instruction)
        analysis["episode_id"] = episode_id
        analysis["instruction"] = instruction
        all_analyses.append(analysis)
        
        # Extract domain information
        types, predicates = extract_objects_and_predicates(analysis)
        actions = generate_actions_from_sequence(analysis)
        
        domain_types.update(types)
        domain_predicates.update(predicates)
        domain_actions.update(actions)
        
        print(f"   âœ… Analysis complete: {len(types)} types, {len(predicates)} predicates, {len(actions)} actions")
    
    # Generate unified domain
    print(f"\n{'=' * 70}")
    print("ðŸ“ Generating unified domain.pddl...")
    domain_pddl = create_pddl_domain(
        list(domain_types),
        list(domain_predicates),
        list(domain_actions)
    )
    
    with open(OUTPUT_DOMAIN_FILE, 'w') as f:
        f.write(domain_pddl)
    print(f"   âœ… Saved domain to: {OUTPUT_DOMAIN_FILE}")
    
    # Generate problem files
    print(f"\nðŸ“ Generating problem files...")
    problem_dir = Path(OUTPUT_PROBLEM_DIR)
    problem_dir.mkdir(exist_ok=True)
    
    for analysis in all_analyses:
        episode_id = analysis.get("episode_id", "unknown")
        problem_pddl = create_pddl_problem(episode_id, analysis)
        
        problem_file = problem_dir / f"problem_{episode_id.replace('+', '_')}.pddl"
        with open(problem_file, 'w') as f:
            f.write(problem_pddl)
        print(f"   âœ… Saved problem: {problem_file.name}")
    
    print(f"\n{'=' * 70}")
    print("âœ… PDDL GENERATION COMPLETE!")
    print(f"   Domain: {OUTPUT_DOMAIN_FILE}")
    print(f"   Problems: {OUTPUT_PROBLEM_DIR}/")
    print("=" * 70)


if __name__ == "__main__":
    main()

